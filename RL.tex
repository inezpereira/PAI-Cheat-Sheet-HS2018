\section{Reinforcement Learning}

\subsection{Model-based RL}

\subsubsection{$\epsilon$ greedy}
With probability $\epsilon$, pick random action. With prob $(1-\epsilon)$, pick best action. If sequence $\epsilon$ satisfies Robbins Monro criteria $\rightarrow$ convergence to optimal policy with prob 1.

\subsubsection{$R_{max}$ algorithm}
\textbf{Input}: starting $x_0$, discount factor $\gamma$.\\
\textbf{Initially}: add fairy tale state $x^*$ to MDP\\
- Set $r(x,a)=R_{max}$ for all states x and actions $a$\\
- Set $P(x^*|x,a)=1$ for all states $x$ and actions $a$\\
- Choose the optimal policy for $r$ and $P$\\
\textbf{Repeat}:
1. Execute policy $\pi$ and, for each visited state/action pair, update $r(x,a)$\\
2. Estimate transition probabilities $P(x^{'}|x,a)$\\
3. If observed 'enough' transitions/rewards, recompute policy $\pi$, according to current model $P$ and $r$.\\
\textbf{"Enough"?} See Hoeffding's inequality. To reduce error $\epsilon$, need more samples $N$.\\
\textbf{Theorem}: With probability $1-\delta$, $R_{max}$ will reach an $\epsilon$-optimal policy in a number of steps that is polynomial in $|X|, |A|, T, 1/\epsilon$ and $log(1/\delta)$. Memory $O(|X^2||A|)$. 

\subsection{Model-free RL: estimate V*(x) directly}
\subsubsection{Q-learning}
$Q(x,a) \leftarrow (1-\alpha_t)Q(x,a) + \alpha_t(r+\gamma \max_{a'}Q(x', a'))$\\
% $V^*(x)=\underset{a}{max}Q*(x,a)$\\
\textbf{Theorem}: If learning rate $\alpha_t$ satisfies: $\sum_t \alpha_t=\infty$ and $\sum_t \alpha_t^2 < \infty$ (Robbins-Monro), and actions are chosen at random, then $Q$ learning converges to optimal $Q^*$ with probability 1.\\
\textbf{Optimistic Q learning:}\\
Initialize: $Q(x,a)=\frac{R_{max}}{1-\gamma}\prod_{t=1}^{T_{init}}(1-\alpha_t)^{-1}$\\
Same convergence time as with $R_{max}$. Memory $O(|X||A|)$. Comp: $O(|A|)$.\\
\textbf{Parametric Q-function approximation}: $Q(x,a;\theta)=\theto^T\phi (x,a)$ to scale to large state spaces. (You can use Deep NN here!)\\
\textbf{SGD for ANNs}: initialize weights. For t = 1,2..., pick a data point (x,y) uniformly at random. Take step in negative gradient direction. (In practise, mini-batches).\\
\textbf{Deep Q Networks}: use CNN to approx Q function.
% Use "experience relay". Clone network to maintain constant "target" values across episodes:
$ L(\theta)=\sum_{(x,a,r,x')\in D}(r+\gamma\underset{a'}{max}Q(x',a';\theto^{old})-Q(x,a;\theto))^2$ \textbf{Double DQN:} current network for evaluating argmax (too optimistic, and you remove $\theta^{old}$ and put $\theta$).

\subsection{Gaussian processes}
A GP is an (infinite) set of random variables (RV), indexed by some set X, i.e., for each x in X, there is a RV $Y_x$ where there exists functions $\mu : X \rightarrow \mathbb{R}$ and $K: X \times X \rightarrow \mathbb{R}$ such that for all: $A \in X, \quad A={x_1,...x_k}$, it holds that $Y_A=[Y_{x_1},...,Y_{x_k}] \sim N(\mu_a, \Sigma_{AA})$, where: $\Sigma_{AA} =$ matrix with all combinations of $K(x_i, x_j)$.
% \[ 
% \left (
%   \begin{tabular}{cccc}
%   K(x_1,x_1) & K(x_1,x_2) & \cdots & K(x_1,x_n) \\
%   \vdots &  &  & \vdots \\
%   K(x_k,x_1) & K(x_k,x_2) & \cdots &K(x_k,x_k)
%   \end{tabular}
% \right )
% \]

% $\mu_A =$\begin{pmatrix}\mu(x_1)\\\mu(x_2)\\\vdots\\\mu(x_k}\end{pmatrix}
K is called kernel (covariance) function (must be symmetric and pd) and $\mu$ is called mean function.
\textbf{Making prediction with GPs:} Suppose $P(f)=GP(f;\mu, K)$ and we observe $y_i=f(\overrightarrow{x_i})+\epsilon_i$, $A=\{\overrightarrow{x_1}:\overrightarrow{x_k}\}$
$P(f(x)|\overrightarrow{x_1}:\overrightarrow{x_k},y_{1:k})=GP(f;\mu ', K')$.  In particular, $P(f(x)|\overrightarrow{x_1}:\overrightarrow{x_k},y_{1:k})=N()f(x);\mu_{x|A}, \sigma^2_{x|a}$, where $\mu_{x|a}=\mu(\overrightarrow{x})+\Sigma_{x,A}(\Sigma_{AA}+\sigma^2I)^{-1}\Sigma^T_{x,A}(\overrightarrow{y_A}-\mu_A)$ and $\sigma^2_{x|a}=K(\overrightarrow{x},\overrightarrow{x})-\Sigma_{x,A}(\Sigma_{AA}+\sigma^2I)^{-1}\Sigma^T_{x,A}$. \textbf{Closed form formulas for prediction!}





